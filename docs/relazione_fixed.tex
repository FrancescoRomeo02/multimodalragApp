\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\geometry{margin=2.5cm}

\title{Architettura RAG Multimodale Avanzata per l'Interrogazione di Documenti PDF Complessi}
\author{Francesco Romeo}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Il presente lavoro presenta lo sviluppo e la valutazione di un'architettura RAG (Retrieval-Augmented Generation) multimodale avanzata per l'interrogazione intelligente di documenti PDF complessi. L'architettura integra tecniche di parsing strutturato tramite Unstructured, embedding multimodali basati su CLIP, indicizzazione vettoriale con Qdrant e generazione di risposte tramite LLM (LLaMA 3.1). Il sistema gestisce efficacemente contenuti testuali, visuali e tabulari, superando le limitazioni dei tradizionali sistemi RAG testuali. I risultati sperimentali su un corpus di 10 articoli scientifici dimostrano una similarità semantica media di 0.721 rispetto al sistema di riferimento Morphik, evidenziando particolare efficacia nei domini tecnico-scientifici. L'architettura modulare proposta rappresenta un contributo significativo al panorama emergente delle architetture RAG multimodali, aprendo prospettive per applicazioni in contesti ad alta densità informativa.
\end{abstract}

\tableofcontents
\newpage

\section{Introduzione}

\subsection{Obiettivo del progetto}
Il presente progetto si propone di progettare e implementare una pipeline completa per la realizzazione di un'architettura RAG multimodale avanzata, specificatamente ottimizzata per l'estrazione e la comprensione di informazioni contenute in documenti PDF complessi, prevalentemente articoli scientifici e documenti tecnici forniti dall'utente. 

L'obiettivo principale è costruire un sistema intelligente che, a partire da un documento contenente testo, immagini, tabelle e diagrammi, sia in grado di rispondere in modo coerente, preciso ed esaustivo a domande formulate in linguaggio naturale, mantenendo la tracciabilità delle fonti e la coerenza semantica tra modalità diverse. 

Il sistema implementato rappresenta un'evoluzione significativa rispetto ai tradizionali sistemi RAG solo testuali, integrando capacità di elaborazione multimodale avanzate attraverso diverse tecnologie:

\begin{itemize} 
    \item \textbf{Analisi multimodale avanzata}: processamento simultaneo di contenuti testuali, visuali e tabulari attraverso: 
    \begin{itemize}
        \item Unstructured per layout detection
        \item OCR (Tesseract) per l'estrazione del testo dalle immagini
        \item Computer vision (YOLO) per il riconoscimento di oggetti
        \item Generazione automatica di caption descrittive tramite LLM
        \item Estrazione e interpretazione intelligente delle strutture tabulari
    \end{itemize} 
    \item \textbf{Ricerca vettoriale semantica}: utilizzo di Qdrant come database vettoriale ad alte prestazioni per la similarità semantica attraverso la distanza del coseno, con soglie di confidenza ottimizzate per tipo di contenuto
    \item \textbf{Integrazione LLM multimodale}: accesso a modelli linguistici (GPT-4o, Claude, LLaMA) tramite API Groq per la generazione di risposte contestualizzate
\end{itemize}

\subsection{Contesto applicativo e motivazione}
Negli ultimi anni, l'esplosione della quantità di documenti digitali contenenti informazioni complesse e, soprattutto, strutturati in modo articolato, ha reso evidente la necessità di sistemi intelligenti in grado di comprenderne il contenuto e di presentare all'utente le informazioni rilevanti. In particolare, contesti ad alta densità informativa come la ricerca scientifica, la pubblica amministrazione, il settore legale e l'industria manifatturiera richiedono strumenti capaci di trasformare grandi corpus di documenti semi-strutturati in basi di conoscenza interrogabili in maniera efficiente e semanticamente coerente.

Tradizionalmente, i motori di ricerca testuali si sono dimostrati inadeguati nel rispondere a domande complesse che coinvolgono più concetti distribuiti tra testo e componenti visive. L'informazione contenuta nelle immagini, ad esempio, non è direttamente interrogabile se non viene preventivamente interpretata e rappresentata in forma semantica. Analogamente, le tabelle spesso codificano relazioni quantitative o pattern che un sistema non strutturato non è in grado di cogliere. 

Di fronte a queste limitazioni, si è delineata la necessità di sistemi RAG multimodali, che combinano embedding semantico, indicizzazione vettoriale e generazione di risposte tramite LLM per produrre output informativi e contestualizzati. L'obiettivo è duplice: da un lato, abilitare un retrieval informativo che sia realmente cross-modale; dall'altro, potenziare la generazione di risposte automatica tramite modelli linguistici di nuova generazione, rendendo l'interazione con grandi collezioni documentali accessibile, efficiente e conoscitivamente utile.

\subsection{Contributi della ricerca}
I principali contributi di questo lavoro includono:

\begin{enumerate}
    \item \textbf{Architettura multimodale unificata}: Progettazione di un sistema end-to-end che integra processing di testo, immagini e tabelle in un unico spazio semantico
    \item \textbf{Pipeline di arricchimento semantico}: Sviluppo di strategie avanzate per l'analisi e la descrizione automatica di contenuti visuali
    \item \textbf{Retrieval adattivo}: Implementazione di meccanismi di intent detection per l'ottimizzazione dinamica dei parametri di ricerca
    \item \textbf{Valutazione comparativa}: Benchmarking sistematico contro Morphik su un corpus diversificato di documenti scientifici
    \item \textbf{Framework estensibile}: Architettura modulare che consente facile integrazione di nuovi modelli e strategie
\end{enumerate}

\section{Lavori Correlati}

\subsection{Sistemi RAG Tradizionali}
I sistemi RAG (Retrieval-Augmented Generation) tradizionali si concentrano principalmente sul processing di contenuti testuali \cite{lewis2020retrieval}. Questi approcci utilizzano tecniche di chunking fisso e embedding testuali per l'indicizzazione, seguiti da retrieval basato su similarità semantica e generazione tramite LLM. Tuttavia, tali sistemi presentano limitazioni significative quando applicati a documenti ricchi di contenuti multimodali.

\subsection{Approcci Multimodali Emergenti}
Recenti sviluppi nel campo hanno introdotto architetture capaci di gestire contenuti multimodali. ColPali \cite{colpali2024} rappresenta un approccio innovativo che opera direttamente a livello di pagina, utilizzando Vision Language Models per proiettare informazioni visive e testuali in uno spazio latente condiviso. Jina Embeddings V4 \cite{jina2024} estende questa filosofia integrando tecniche Matryoshka per embedding multi-dimensionali.

\subsection{Modelli di Embedding Multimodali}
CLIP \cite{radford2021learning} ha rivoluzionato il campo degli embedding multimodali attraverso l'addestramento contrastivo su coppie testo-immagine. Le varianti multilingue come clip-ViT-B-32-multilingual-v1 estendono queste capacità a contesti internazionali, rendendole particolarmente adatte per applicazioni documentali eterogenee.

\section{Metodologia}

\subsection{Architettura del Sistema}

\subsubsection{Panoramica dell'architettura}
L'architettura di MultimodalRAG è stata progettata seguendo principi di modularità, scalabilità e separazione delle responsabilità, con l'obiettivo di creare un sistema robusto e facilmente manutenibile per l'elaborazione di documenti multimodali. Il sistema si articola in diversi livelli architetturali che collaborano per fornire un'esperienza di interrogazione intelligente e contestualizzata.

\begin{figure}[!ht]
\centering
\includegraphics[width=1\textwidth]{figures/diagram.png}
\caption{Architettura RAG multimodale proposta}
\label{fig:architecture}
\end{figure}

\subsubsection{Pipeline di elaborazione}
L'architettura implementa una pipeline di elaborazione end-to-end che esegue:

\begin{enumerate}
    \item \textbf{Parsing approfondito del documento}: viene effettuata un'analisi strutturale ad alta risoluzione del PDF tramite la libreria \verb|unstructured|, con l'ausilio della strategia \verb|hi_res| per preservare l'integrità semantica e gerarchica del documento. I blocchi testuali vengono segmentati secondo una logica orientata al contenuto (e.g., titoli, paragrafi, sezioni), mentre elementi tabellari e grafici vengono identificati separatamente e gestiti come entità autonome.
    
    \item \textbf{Analisi visiva delle componenti multimodali}: le immagini vengono arricchite semanticamente tramite una pipeline multimodale che include la generazione di caption con modelli VLMs, OCR avanzato tramite Tesseract, e rilevamento oggetti con YOLOv8. Analogamente, le tabelle sono sintetizzate mediante modelli LLM (Groq) in grado di generare un riassunto informativo a partire dalla loro rappresentazione HTML.
    
    \item \textbf{Chunking adattivo}: il testo viene segmentato in unità semantiche coerenti tramite una strategia di chunking dinamica basata sui titoli (\verb|by_title|), che consente di mantenere l'integrità tematica dei contenuti ed evitare spezzature arbitrarie. La strategia adottata prevede la combinazione di blocchi testuali troppo brevi e la creazione di nuovi chunk dopo un certo numero di caratteri, garantendo così un bilanciamento tra granularità e contesto.
    
    \item \textbf{Embedding}: ogni blocco testuale, immagine o tabella viene convertito in un embedding vettoriale tramite un modello multimodale CLIP: "sentence-transformers/clip-ViT-B-32-multilingual-v1"
    
    \item \textbf{Indicizzazione semantica}: i vettori così ottenuti vengono indicizzati in un database vettoriale (Qdrant) insieme ai metadati strutturati e contestualizzati.
    
    \item \textbf{Retrieval}: il recupero dei contenuti avviene tramite una ricerca per similarità vettoriale con soglie dinamiche adattate al tipo di contenuto (testo, immagine, tabella) e all'intento dell'utente (esplorativo, tecnico, fattuale, multimodale). Viene applicato un filtro intelligente basato su metadati come il file di origine o il tipo semantico del contenuto.
    
    \item \textbf{Generazione}: i risultati recuperati vengono integrati e sintetizzati in risposte informative e strutturate tramite LLMs.
\end{enumerate}

L'integrazione di queste tecnologie consente al sistema di superare le limitazioni dei tradizionali approcci RAG testuali, offrendo un'esperienza di interrogazione più ricca e semanticamente coerente per documenti complessi come paper scientifici.

\subsubsection{Stack tecnologico}
L'architettura è supportata da uno stack tecnologico robusto. \textbf{Qdrant} fornisce il database vettoriale ad alte prestazioni, ottimizzato per ricerche di similarità su larga scala con supporto per filtri complessi. \textbf{LangChain} orchestra l'intera pipeline RAG, offrendo un framework consolidato per l'integrazione di componenti di elaborazione del linguaggio naturale.

Per l'elaborazione multimodale, il sistema integra \textbf{Unstructured} per il parsing avanzato dei PDF, \textbf{Tesseract} per l'OCR, \textbf{YOLOv8} per l'object detection e un modello CLIP per l'embedding dei chunk creati. L'interfaccia utente sfrutta \textbf{Streamlit} per fornire un'esperienza web moderna e responsive.

Il deployment è completamente containerizzato attraverso \textbf{Docker} e Docker Compose. Il Dockerfile implementa una build multi-stage che ottimizza le dimensioni dell'immagine finale, includendo tutte le dipendenze di sistema necessarie (Tesseract, Poppler, librerie di computer vision). Il sistema può essere lanciato con un singolo comando \verb|docker-compose up -d|, che orchestra automaticamente l'avvio di tutti i servizi necessari.

La configurazione è centralizzata in \texttt{config.py}, che definisce parametri ottimizzati per diversi scenari d'uso e consente facile tuning delle performance. Il sistema include inoltre robusti meccanismi di logging e monitoraggio delle performance.

L'architettura modulare consente facile estensibilità: nuovi tipi di modelli di embedding possono essere integrati modificando solo l'AdvancedEmbedder, nuove strategie di chunking possono essere aggiunte senza impattare gli altri componenti, e nuovi fornitori di LLM possono essere supportati estendendo il modulo di integrazione in llm.

\subsection{Preprocessing del Documento}

\subsubsection{Estrazione strutturata del contenuto}
L'estrazione degli elementi in maniera corretta e precisa costituisce un aspetto fondamentale per lo sviluppo di un'architettura RAG multimodale efficace. In questo progetto è stata orchestrata attraverso l'utilizzo della libreria Unstructured, che consente di superare i limiti dei parser tradizionali fornendo una visione semantica e strutturata dei documenti.

L'obiettivo è stato quello di individuare testo, estrarre immagini e tabelle e di riconoscere e segmentare porzioni logicamente visibili e visivamente coerenti trattandole come elementi informativi.

La funzione principale, \textbf{parse\_pdf\_elements}, utilizza il metodo \textbf{partition\_pdf} di \textbf{Unstructured} ricevendo in input il percorso del file da analizzare e sfruttando una combinazione di OCR, analisi layout e strutturazione semantica. Con l'opzione \verb|hi_res|, il documento viene analizzato combinando riconoscimento del layout visuale e strutturazione logica.

\begin{figure}[!ht]
\centering
\includegraphics[width=1\textwidth]{figures/metadata text.png}
\caption{Metadati estratti per elementi testuali}
\label{fig:text_metadata}
\end{figure}

\subsubsection{Gestione delle immagini}
Per le immagini, oltre alla semplice estrazione base64, viene attivata una pipeline di arricchimento semantico: un modello visivo genera una descrizione automatica del contenuto, mentre altri moduli identificano oggetti visivi e testo presente nell'immagine. Tutte queste informazioni vengono poi fuse in un'unica stringa descrittiva, che diventa a tutti gli effetti il contenuto semantico su cui il sistema potrà eseguire ricerca e retrieval.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.75\linewidth]{figures/metadata.png}
\caption{Struttura dei metadati per le immagini in Qdrant}
\label{fig:image_metadata}
\end{figure}

\subsubsection{Gestione delle tabelle}
Per quanto riguarda le tabelle, viene attivato il supporto alla ricostruzione HTML e, se configurato, un ulteriore modello LLM che genera un riassunto interpretativo del contenuto tabellare, descrivendone tipologia, trend principali e valori salienti.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.75\linewidth]{figures/metadata table.png}
\caption{Metadati delle tabelle estratte}
\label{fig:table_metadata}
\end{figure}

\subsection{Generazione degli Embedding Multimodali}
Dopo l'estrazione e la tipizzazione degli elementi dal documento PDF, ogni componente viene convertito in una rappresentazione densa (embedding vettoriale) che ne cattura il significato semantico, visivo o strutturale. Questo passaggio è fondamentale per permettere operazioni di ricerca basate su similarità semantica all'interno di uno spazio vettoriale condiviso.

\subsubsection{Modello di embedding}
Il sistema utilizza \textbf{sentence-transformers/clip-ViT-B-32-multilingual-v1}, che rappresenta una fusione tra le architetture CLIP (Contrastive Language–Image Pretraining) di OpenAI e le Sentence Transformers, adattata per supportare più lingue e input multimodali.

La caratteristica distintiva del modello è la proiezione di testi e immagini in un unico spazio semantico, che permette di confrontare direttamente contenuti visivi e testuali, calcolandone la similarità con una metrica uniforme.

\begin{figure}[!ht]
\centering
\includegraphics[width=1\textwidth]{figures/embedding.png}
\caption{Rappresentazione dello spazio di embedding multimodale}
\label{fig:embedding_space}
\end{figure}

\subsection{Indicizzazione e Storage}

\subsubsection{Qdrant Database}
Qdrant è un motore di ricerca vettoriale ottimizzato per applicazioni di information retrieval basate su rappresentazioni dense e distribuzionali dei dati. Progettato per scenari reali ad alta scala, Qdrant consente di eseguire query di similarità k-NN su insiemi di vettori multidimensionali, preservando metadati strutturati e supportando filtri booleani complessi.

Il modulo \verb|QdrantManager| gestisce la connessione e l'interazione con il database vettoriale. La collezione supporta configurazioni persistenti su disco (\verb|on_disk=True|), consentendo lo storage anche su grandi volumi di dati.

\subsection{Sistema di Retrieval}

\subsubsection{Analisi dell'intento e parametri adattivi}
Il processo di retrieval inizia con una fase di \textit{intent detection}, che classifica automaticamente la natura semantica della domanda. Il classificatore utilizza euristiche lessicali per determinare se la richiesta è di tipo:

\begin{itemize}
    \item \textbf{Fattuale}: domande dirette o definizioni
    \item \textbf{Tecnico}: contenuti con codice, API, configurazioni  
    \item \textbf{Esplorativo}: richieste generiche o narrative
    \item \textbf{Multimodale}: riferimenti a grafici, immagini, tabelle
\end{itemize}

\subsubsection{Retrieval vettoriale}
Dopo la classificazione semantica dell'intento, il sistema avvia una ricerca vettoriale tramite il gestore \verb|qdrant_manager|, utilizzando come chiave di ricerca il vettore generato dalla query stessa. Il retriever restituisce contenuti testuali, immagini e tabelle ordinati per similarità semantica.

\subsection{Generazione delle Risposte}

\subsubsection{Modello generativo}
L'LLM impiegato è \textbf{meta-llama/llama-3.1-8b-instruct}, un modello di casa Meta appartenente alla famiglia LLaMA 3.1. Si tratta di una variante instruction-tuned basata su 8 miliardi di parametri, progettata per eccellere nei task di ragionamento guidato, question answering complesso e sintesi multi-sorgente.

\subsubsection{Prompt engineering}
Il prompt engineering rappresenta il ponte tra il contenuto recuperato dal sistema di retrieval e la capacità del modello generativo di produrre una risposta coerente. Il prompt è strutturato in sezioni operative che contribuiscono a migliorare la robustezza e la trasparenza del sistema.

\begin{figure}[!ht]
\centering
\includegraphics[width=1\textwidth]{figures/prompt.png}
\caption{Struttura del prompt utilizzato per la generazione}
\label{fig:prompt_structure}
\end{figure}

\section{Risultati Sperimentali}

\subsection{Setup Sperimentale}
Per la valutazione del sistema sono stati utilizzati 10 articoli scientifici appartenenti a domini diversi: matematica, economia, fisica, geografia e informatica. Il sistema di riferimento per il benchmark è \textbf{Morphik}, un framework AI che si posizionava come progetto open-source leader al momento delle sperimentazioni.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{figures/morphik.png}
\caption{Configurazione del sistema Morphik per il benchmark}
\label{fig:morphik_config}
\end{figure}

\subsection{Dataset e Metriche}
I benchmark sono stati condotti su un campione di 100 domande (10 per ogni paper), classificate per difficoltà crescente. Le metriche di valutazione si sono concentrate su:

\begin{itemize}
    \item \textbf{Completezza}: capacità del sistema di coprire tutti gli aspetti rilevanti della query
    \item \textbf{Chiarezza}: comprensibilità e coerenza delle risposte fornite
    \item \textbf{Correttezza}: accuratezza fattuale delle informazioni riportate
    \item \textbf{Pertinenza}: rilevanza delle risposte rispetto alle domande poste
\end{itemize}

\subsection{Analisi delle Performance}

\subsubsection{Distribuzione globale delle performance}
\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{graph/04_distribuzione_performance.png}
\caption{Distribuzione delle performance globali del sistema}
\label{fig:global_performance}
\end{figure}

Il grafico mostra la distribuzione delle risposte in diverse fasce di performance:

\begin{itemize}
    \item \textbf{Eccellente} (\(\geq0.85\)): risposte pressoché identiche a quelle generate da Morphik
    \item \textbf{Buono} (\(\geq0.7\)): risposte valide con lievi differenze ma con contesto adeguatamente catturato
    \item \textbf{Soglia Critica} (\(\geq0.3\)): risposte sostanzialmente diverse da quelle generate da Morphik
    \item \textbf{Insufficiente} (\(<0.3\)): risposte non simili per contesto non catturato
\end{itemize}

La media delle similarità semantiche è pari a $0.721$, indicando una \textbf{buona performance complessiva} del modello.

\subsubsection{Performance per livello di difficoltà}
\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{graph/01_performance_per_difficolta.png}
\caption{Analisi delle performance per livello di difficoltà}
\label{fig:difficulty_performance}
\end{figure}

Il grafico dimostra che il sistema mantiene una similarità media relativamente elevata su tutti i livelli di difficoltà, indicando robustezza architetturale anche per domande complesse che richiedono captioning preciso delle risorse multimodali.

\subsubsection{Performance per dominio}
\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{graph/02_performance_per_argomento.png}
\caption{Performance per macro-argomento}
\label{fig:domain_performance}
\end{figure}

L'analisi per dominio rivela:

\begin{itemize}
    \item \textbf{Performance superiori}: Matematica (0.76), Ambiente (0.75), Ingegneria e Tecnologia (0.74)
    \item \textbf{Performance inferiori}: Geografia (0.60), potenzialmente dovuta alla complessità dei contenuti visivi geografici
    \item \textbf{Performance intermedie}: Energia (0.67), Generale (0.72)
\end{itemize}

\subsubsection{Analisi dettagliata con heatmap}
\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{graph/03_heatmap_argomento_difficolta.png}
\caption{Heatmap della performance per dominio e difficoltà}
\label{fig:heatmap_analysis}
\end{figure}

La heatmap fornisce un'analisi granulare delle performance, evidenziando come l'architettura si comporti in funzione sia del dominio che della complessità delle query.

\subsection{Discussione dei Risultati}
I risultati ottenuti dimostrano l'efficacia dell'approccio multimodale proposto:

\begin{enumerate}
    \item \textbf{Robustezza cross-dominio}: Il sistema mantiene performance consistenti across diversi domini scientifici
    \item \textbf{Gestione della complessità}: Performance stabili anche per query complesse che richiedono reasoning multimodale
    \item \textbf{Similarità semantica elevata}: Media di 0.721 indica forte allineamento con il sistema di riferimento
    \item \textbf{Punti di forza identificati}: Particolare efficacia in domini tecnico-quantitativi (matematica, ingegneria)
    \item \textbf{Aree di miglioramento}: Necessità di ottimizzazione per contenuti geografici/visuali complessi
\end{enumerate}

\section{Approcci Alternativi}

\subsection{Approccio ColPali}
Un contributo rilevante nel campo è rappresentato da ColPali, che introduce un approccio innovativo per la gestione multimodale dei PDF. Il documento viene scomposto a livello di pagina, con ogni pagina segmentata in regioni visive distinte elaborate da Vision Language Models.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.75\linewidth]{figures/colpali.png}
\caption{Confronto tra approccio standard e ColPali}
\label{fig:colpali_comparison}
\end{figure}

\subsection{Jina Embeddings V4}
Jina Embeddings V4 rappresenta un modello avanzato con circa 3.8 miliardi di parametri, che integra l'approccio ColPali per la proiezione di feature visive nello spazio latente testuale. Il modello supporta modalità multi-vector secondo la tecnica Matryoshka.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.75\linewidth]{figures/jina.png}
\caption{Architettura Jina Embeddings V4}
\label{fig:jina_architecture}
\end{figure}

\section{Limitazioni e Lavori Futuri}

\subsection{Limitazioni Attuali}
L'architettura proposta presenta alcune limitazioni che meritano discussione:

\begin{itemize}
    \item \textbf{Dipendenza dalla qualità del parsing}: L'efficacia dipende dalla capacità di Unstructured di interpretare correttamente layout complessi
    \item \textbf{Limitazioni nell'embedding delle immagini}: Perdita potenziale di informazioni visive nella conversione testuale
    \item \textbf{Scalabilità computazionale}: Requisiti computazionali significativi per documenti ricchi di contenuti visuali
    \item \textbf{Gestione di relazioni inter-modali}: Mancanza di cattura esplicita delle relazioni semantiche cross-modali
    \item \textbf{Valutazione limitata}: Benchmark su corpus relativamente ristretto
\end{itemize}

\subsection{Direzioni Future}
Sulla base dell'analisi condotta si delineano diverse direzioni per lavori futuri:

\begin{itemize}
    \item \textbf{Integrazione di approcci end-to-end}: Adozione di modelli come ColPali per embedding diretto delle pagine
    \item \textbf{Miglioramento del retrieval cross-modale}: Sviluppo di strategie che considerino relazioni inter-modali esplicite
    \item \textbf{Ottimizzazione per domini specifici}: Fine-tuning su corpus specialistici per domini tecnici
    \item \textbf{Valutazione estesa}: Benchmark su dataset più ampi e diversificati
    \item \textbf{Integrazione di memoria episodica}: Capacità di mantenere contesto conversazionale per sessioni prolungate
    \item \textbf{Ottimizzazione dell'efficienza}: Tecniche di compressione e pruning per deployment edge
\end{itemize}

\section{Conclusioni}

Il presente lavoro ha presentato lo sviluppo e la valutazione di un'architettura RAG multimodale avanzata per l'interrogazione di documenti PDF complessi. L'integrazione di tecniche di parsing strutturato, embedding multimodali e generazione guidata da LLM ha dimostrato risultati promettenti, con una similarità semantica media di 0.721 rispetto al sistema di riferimento Morphik.

\subsection{Contributi Principali}
I principali contributi di questa ricerca includono:

\begin{enumerate}
    \item \textbf{Architettura multimodale unificata}: Design end-to-end per processing integrato di contenuti eterogenei
    \item \textbf{Pipeline di arricchimento semantico}: Strategie avanzate per analisi automatica di contenuti visuali
    \item \textbf{Retrieval adattivo}: Meccanismi di intent detection per ottimizzazione dinamica
    \item \textbf{Validazione empirica}: Benchmarking sistematico su corpus diversificato
    \item \textbf{Framework estensibile}: Architettura modulare per facile integrazione di nuovi componenti
\end{enumerate}

\subsection{Impatto e Applicazioni}
L'architettura modulare proposta consente una gestione efficace di contenuti testuali, visuali e tabulari, superando le limitazioni dei sistemi RAG tradizionalmente testuali. I risultati del benchmark evidenziano particolare efficacia nei domini tecnico-scientifici, confermando il potenziale dell'approccio per applicazioni reali in contesti ad alta densità informativa.

Le direzioni future identificate, in particolare l'adozione di approcci end-to-end e il miglioramento del retrieval cross-modale, aprono prospettive interessanti per ulteriori sviluppi del sistema, posizionandolo come una soluzione competitiva nel panorama emergente delle architetture RAG multimodali.

\subsection{Considerazioni Finali}
Il lavoro dimostra la fattibilità e l'efficacia di architetture RAG multimodali per applicazioni documentali complesse. La combinazione di tecniche state-of-the-art per processing multimodale, retrieval semantico e generazione linguistica apre nuove possibilità per l'interrogazione intelligente di grandi corpus documentali, con implicazioni significative per ricerca scientifica, analisi documentale enterprise e sistemi di knowledge management avanzati.

\section*{Ringraziamenti}
Si ringrazia il corso di Algoritmi Distribuiti per il supporto metodologico e l'accesso alle risorse computazionali necessarie per la realizzazione di questo progetto.

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{lewis2020retrieval}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... \& Kiela, D. (2020). 
\textit{Retrieval-augmented generation for knowledge-intensive nlp tasks}. 
Advances in neural information processing systems, 33, 9459-9474.

\bibitem{radford2021learning}
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... \& Sutskever, I. (2021). 
\textit{Learning transferable visual models from natural language supervision}. 
In International conference on machine learning (pp. 8748-8763).

\bibitem{colpali2024}
ColPali Team. (2024). 
\textit{ColPali: Efficient Document Retrieval with Vision Language Models}. 
arXiv preprint arXiv:2407.01449.

\bibitem{jina2024}
Jina AI. (2024). 
\textit{Jina Embeddings V4: Multimodal Embeddings for Multimodal Applications}. 
Technical Report.

\bibitem{qdrant2023}
Qdrant Team. (2023). 
\textit{Qdrant: Vector Database for the Next Generation of AI Applications}. 
Documentation and Technical Specifications.

\bibitem{unstructured2023}
Unstructured Team. (2023). 
\textit{Unstructured: Open Source Data Processing for LLMs}. 
GitHub Repository and Documentation.

\bibitem{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... \& Scialom, T. (2023). 
\textit{Llama 2: Open foundation and fine-tuned chat models}. 
arXiv preprint arXiv:2307.09288.

\bibitem{morphik2024}
Morphik Team. (2024). 
\textit{Morphik: Open Source Multimodal RAG Framework}. 
GitHub Repository.

\bibitem{tesseract2021}
Smith, R. (2021). 
\textit{Tesseract OCR: Open Source Optical Character Recognition}. 
Google Open Source Documentation.

\end{thebibliography}

\end{document}
