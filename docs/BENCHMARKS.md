# RAG Benchmark Analysis - Sistema di Analisi Correttezza

Questo progetto implementa un sistema completo per l'analisi dei risultati di benchmark RAG, incluso il calcolo del tasso di correttezza del modello locale rispetto a un modello di riferimento (Morphik), utilizzando SBERT per confronti semantici e identificazione di aree di fallimento e successo.

## ğŸ“ Struttura del Progetto

```
bench_script/
â”œâ”€â”€ bench_res/                         # Directory con i file JSON di benchmark
â”‚   â”œâ”€â”€ rag_bench_pd1.json            # Risultati benchmark documento 1
â”‚   â”œâ”€â”€ rag_bench_pd2.json            # Risultati benchmark documento 2
â”‚   â”œâ”€â”€ rag_bench_pd3.json            # Risultati benchmark documento 3
â”‚   â””â”€â”€ ... (altri file benchmark)
â”œâ”€â”€ bench_file/                        # Directory con i PDF originali
â”‚   â”œâ”€â”€ pdf_01.pdf
â”‚   â”œâ”€â”€ pdf_02.pdf
â”‚   â””â”€â”€ ... (documenti fonte)
â”œâ”€â”€ plots/                             # Grafici di analisi generati
â”‚   â”œâ”€â”€ 01_performance_per_difficolta.png      # Performance vs difficoltÃ 
â”‚   â”œâ”€â”€ 02_performance_per_argomento.png       # Performance per argomento
â”‚   â”œâ”€â”€ 03_heatmap_argomento_difficolta.png    # Interazione difficoltÃ -argomento
â”‚   â”œâ”€â”€ 04_distribuzione_performance.png       # Distribuzione generale
â”‚   â”œâ”€â”€ 05_correlazione_chunk_performance.png  # Correlazione retrieval-risposta
â”‚   â”œâ”€â”€ 06_lunghezza_per_qualita.png          # Analisi lunghezza risposte
â”‚   â”œâ”€â”€ 07_performance_per_tipo_domanda.png    # Performance per tipo domanda
â”‚   â”œâ”€â”€ 08_coverage_termini_tecnici.png       # Coverage terminologia
â”‚   â”œâ”€â”€ 09_casi_non_risposta.png              # Fallimenti totali
â”‚   â”œâ”€â”€ 10_peggiori_performance.png           # Peggiori performance effettive
â”‚   â”œâ”€â”€ 11_distribuzione_per_difficolta.png   # Boxplot per difficoltÃ 
â”‚   â”œâ”€â”€ 12_performance_per_paper.png          # Performance per documento*
â”‚   â””â”€â”€ 13_variabilita_per_paper.png          # VariabilitÃ  per documento*
â”œâ”€â”€ correctness_analysis.py            # Script principale di analisi
â”œâ”€â”€ correctness_analysis_detailed.csv  # Risultati dettagliati esportati
â”œâ”€â”€ CORRECTNESS_REPORT.md             # Report markdown generato
â”œâ”€â”€ GUIDA_GRAFICI_COMPLETA.md         # Guida completa ai grafici
â””â”€â”€ README.md                         # Questo file

* = Generati solo con piÃ¹ documenti
```

## ğŸš€ Installazione e Setup

1. **Preparazione ambiente:**
   ```bash
   cd bench_script
   python -m venv .venv
   source .venv/bin/activate  # macOS/Linux
   # oppure su Windows: .venv\Scripts\activate
   ```

2. **Installazione dipendenze:**
   ```bash
   pip install sentence-transformers numpy pandas matplotlib seaborn scikit-learn
   ```

## ğŸ“Š Utilizzo

### Analisi Completa (Raccomandato)
```bash
python correctness_analysis.py
```

**Il script esegue automaticamente:**
-  Analisi della qualitÃ  delle risposte tramite SBERT
-  Classificazione in livelli di correttezza
-  Identificazione di pattern di fallimento e successo
-  Generazione di 13 grafici individuali
-  Report dettagliato in markdown

**Output generati:**
- `correctness_analysis_detailed.csv` - Dati dettagliati per ogni domanda
- `CORRECTNESS_REPORT.md` - Report completo con metriche e raccomandazioni
- `plots/01_*.png` fino a `plots/13_*.png` - Grafici di analisi individuali

### Interpretazione dei Risultati

**Consultare i grafici in questo ordine:**
1. `04_distribuzione_performance.png` - Overview generale del sistema
2. `01_performance_per_difficolta.png` - Gestione della complessitÃ 
3. `02_performance_per_argomento.png` - Domini problematici
4. `05_correlazione_chunk_performance.png` - Diagnosi retrieval vs generazione
5. `09_casi_non_risposta.png` - Fallimenti critici

**Per guida dettagliata:** Consultare `GUIDA_GRAFICI_COMPLETA.md`

## ğŸ“Š Formato Dati di Input

### Struttura JSON Richiesta:
```json
{
  "file": "nome_paper.pdf",
  "questions": [
    {
      "question_id": "q1",
      "question": "Testo della domanda",
      "Morphik": {
        "response": "Risposta modello benchmark",
        "chunks": [0, 3, 1, 16, 5]  // Indicizzazione da 0
      },
      "local": {
        "response": "Risposta modello locale",
        "chunks": [1, 3, 5, 6, 11]  // Indicizzazione da 1 (auto-convertita)
      }
    }
  ]
}
```

## ğŸ“Š Metriche e Soglie

### Classificazione della Correttezza:
- **Eccellente**: â‰¥ 0.85 (QualitÃ  molto alta)
- **Buono**: â‰¥ 0.70 (QualitÃ  accettabile)
- **Accettabile**: â‰¥ 0.50 (Necessita miglioramenti)
- **Scarso**: â‰¥ 0.30 (Problematico)
- **Molto Scarso**: < 0.30 (Critico)

### KPI di Sistema:
- **Tasso Buono+**: Target â‰¥ 50%
- **SimilaritÃ  Media**: Target â‰¥ 0.60
- **Casi Critici**: Target â‰¤ 10%
- **Correlazione Retrieval**: Target â‰¥ 0.40

### Metriche Calcolate:
- **SimilaritÃ  Semantica**: Confronto SBERT tra risposte
- **QualitÃ  Retrieval**: F1, Precision, Recall sui chunk
- **Analisi Linguistica**: Lunghezza, terminologia tecnica
- **Pattern Analysis**: Tipo domanda, difficoltÃ , argomento

## Checklist di Controllo QualitÃ 

**Sistema Sano:**
- [ ] SimilaritÃ  media > 0.60
- [ ] Tasso Buono+ > 50%
- [ ] Correlazione retrieval-performance > 0.4
- [ ] Pochi casi di non-risposta (< 5%)

**Richiede Attenzione:**
- [ ] SimilaritÃ  media 0.40-0.60
- [ ] Alcuni argomenti problematici
- [ ] Alta variabilitÃ  tra documenti

**Critico:**
- [ ] SimilaritÃ  media < 0.40
- [ ] Molti casi di non-risposta
- [ ] Nessuna correlazione retrieval-performance
